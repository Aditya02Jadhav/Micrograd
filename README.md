# Micrograd

Created a neural network from scratch imitating PyTorch libraryâ€™s under-the-hood functions for  forward and backward pass. \
Implemented gradient calculation using both manual (one by one) and automatic (function) methods.


## PrePrerequisites

Before running the Jupyter Notebook, make sure you have the following libraries installed:

Math \
NumPy   
Matplotlib \
PyTorch \
Random 


## Getting Started

Clone the Repository

```bash
  git clone https://github.com/yourusername/your-repo-name.git
  cd your-repo-name
```
Install the Required Libraries

```bash
  pip install numpy matplotlib torch
```
Run the Jupyter Notebook

```bash
  jupyter notebook
```
Execute the Notebook Cells


## License
This project is licensed under the [MIT](https://github.com/git/git-scm.com/blob/main/MIT-LICENSE.txt) License - see the LICENSE file for details.


## Acknowledgements

 - [Jupyter](https://jupyter.org/)
 - [NumPy](https://numpy.org/)
 - [Matplotlib](https://matplotlib.org/)
 - [PyTorch](https://pytorch.org/)
 - [Python](https://www.python.org/)
